# vLLM Manager Configuration

# Server Port
PORT=3001

# Node Environment (development, production)
NODE_ENV=development

# Server Hostname/IP for vLLM instances
# Change this to your server's IP or hostname in production
DEFAULT_HOSTNAME=inference.vm

# HuggingFace API Key (optional, for accessing gated models)
# Get your token from https://huggingface.co/settings/tokens
# HF_TOKEN=your_huggingface_token_here

# Database Configuration
DB_PATH=./server/data/vllm.db

# Docker Configuration
DOCKER_SOCKET_PATH=/var/run/docker.sock

# Port Range for vLLM Instances
MIN_PORT=8001
MAX_PORT=9000

# Default vLLM Settings
DEFAULT_API_KEY=localkey
VLLM_IMAGE=vllm/vllm-openai:latest

# Security Settings
# JWT_SECRET=your_jwt_secret_here_in_production
# SESSION_SECRET=your_session_secret_here_in_production

# Logging
LOG_LEVEL=info

# Rate Limiting (optional - defaults are environment-based)
# RATE_LIMIT_GENERAL_MAX=1000
# RATE_LIMIT_API_MAX=10000
# RATE_LIMIT_STRICT_MAX=100
# RATE_LIMIT_WINDOW_MS=900000

# CORS Configuration (for production)
# FRONTEND_URL=https://your-frontend-domain.com 